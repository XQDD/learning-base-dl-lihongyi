{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "\n",
    "# 准备数据\n",
    "def read_file(path, label=True):\n",
    "    \"\"\"读取图片文件\"\"\"\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    # 图片数据形式，3为rgb3通道\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    # 遍历列表\n",
    "    for i, file in enumerate(image_dir):\n",
    "        # 原图片是512*512，重新调整大小为128*128\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        x[i, :, :] = cv2.resize(img, (128, 128))\n",
    "        # 若包含分类信息，则设置分类信息\n",
    "        if label:\n",
    "            y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "        return x, y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "train_x, train_y = read_file(\"./resource/training\")\n",
    "val_x, val_y = read_file(\"./resource/validation\")\n",
    "test_x = read_file(\"./resource/testing\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 预处理图片数据\n",
    "train_transform=transforms.Compose([\n",
    "    # PIL图像 python image library，python的知名第三方图形处理库\n",
    "    transforms.ToPILImage(),\n",
    "    # 随机水平翻转图片\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # 随机旋转图片\n",
    "    transforms.RandomRotation(15),\n",
    "    # 将图片转为pytorch的tensor，并且自动把数值normalize到[0,1]\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 继承于Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    # 构造函数，python没有提供默认的this指针，需要自动手动传一个self\n",
    "    def __init__(self,x,y=None,transform=None):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        if y is not None:\n",
    "            # 将numpy array格式的y转为pytorch的tensor\n",
    "            self.y=torch.LongTensor(y)\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        _x=self.x[index]\n",
    "        if self.transform is not None:\n",
    "            _x=self.transform(_x)\n",
    "        if self.y is not None:\n",
    "            _y=self.y[index]\n",
    "            return _x,_y\n",
    "        else:\n",
    "            return _x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_set=ImageDataset(train_x,train_y,train_transform)\n",
    "val_set=ImageDataset(val_x,val_y,test_transform)\n",
    "# dataloader：数据迭代器，实现批量(batch)读取，打乱数据(shuffle)，并提供并行加速等功能\n",
    "train_loader=DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "val_loader=DataLoader(val_set,batch_size=batch_size,shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model\n",
    "# 分类器，继承于nn.Module\n",
    "# 5层卷积层，3层全连接层，这里应该是AlexNet模型\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.cnn=nn.Sequential(\n",
    "            # 卷积层，所以此处用的是cnn，作用可能是匹配出某一模式(比如检测是否是连续的，方向是向上还是向右的)，相对传统全连接神经网络，好处是参数少，计算量小，不容易过拟合\n",
    "            # 最开始输入维度是3*128*128，这里输出维度为64，应该是会自动算出所要求的卷积核的规模（最终目的是计算出这些卷积核或过滤器的数值）\n",
    "            nn.Conv2d(3,64,3,1,1),\n",
    "            # 归一化处理，这使得数据在进行Relu之前不会因为数据过大而导致网络性能的不稳定\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 激活函数用ReLU，一般比sigmoid好\n",
    "            nn.ReLU(),\n",
    "            # 进行MaxPool进行池化，又称下采样，让神经网络拥有抽象能力？\n",
    "            # 池化层在CNN中可用来减小尺寸，提高运算速度及减小噪声影响，让各特征更具有健壮性。池化层比卷积层更简单，它没有卷积运算，只是在滤波器算子滑动区域内取最大值或平均值。而池化的作用则体现在降采样：保留显著特征、降低特征维度，增大感受野。深度网络越往后面越能捕捉到物体的语义信息，这种语义信息是建立在较大的感受野基础上\n",
    "            nn.MaxPool2d(2,2,0),\n",
    "\n",
    "            nn.Conv2d(64,128,3,1,1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),\n",
    "\n",
    "            nn.Conv2d(128,256,3,1,1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),\n",
    "\n",
    "            nn.Conv2d(256,512,3,1,1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),\n",
    "\n",
    "            nn.Conv2d(512,512,3,1,1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0), #最后得到的结果是512*4*4\n",
    "        )\n",
    "\n",
    "        # flatten，拉直\n",
    "        self.fc=nn.Sequential(\n",
    "            # 全连接层\n",
    "            # 输入是512*4*4，输出1024\n",
    "            nn.Linear(512*4*4,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            # 最终拉成11分类的结果\n",
    "            nn.Linear(512,11),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out=self.cnn(x)\n",
    "        out=out.view(out.size()[0],-1)\n",
    "        return self.fc(out)\n",
    "\n",
    "# 训练(使用gpu)\n",
    "model=Classifier().cuda()\n",
    "# 因为是分类问题，所以loss函数使用CrossEntropy\n",
    "loss=nn.CrossEntropyLoss()\n",
    "# 优化使用adam\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "num_epoch=30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time=time.time()\n",
    "    train_acc=0.0\n",
    "    train_loss=0.0\n",
    "    val_acc=0.0\n",
    "    val_loss=0.0\n",
    "    # 相关资料可参考：https://www.cnblogs.com/pinard/p/6418668.html\n",
    "    # 层、模型、损失函数和优化器等都定义或创建好，接下来就是训练模型。训练模型时需要注意使模型处于训练模式，即调用model.train()。调用 model.train()会把所有的module设置为训练模式。如果是测试或验证阶段， 需要使模型处于验证阶段，即调用model.eval()，调用model.eval()会把所有的training属性设置为False。缺省情况下梯度是累加的，需要手工把梯度初始化或清零，调用optimizer.zero_grad()即可。训练过程中，正向传播生成网络的输出，计算输 出和实际值之间的损失值。调用loss.backward()自动生成梯度，然后使用 optimizer.step() 执行优化器，把梯度传播回每个网络\n",
    "    model.train()\n",
    "    # 分批训练\n",
    "    for i,data in enumerate(train_loader):\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播，可以计算出预测结果\n",
    "        train_pred = model(data[0].cuda())\n",
    "        # 反向传播，就是计算微分，即用于求loss/cost function的最小值，用于更新参数\n",
    "        batch_loss=loss(train_pred,data[1].cuda())\n",
    "        batch_loss.backward()\n",
    "        # 紧跟着优化\n",
    "        optimizer.step()\n",
    "        # 计算预测精度，argmax：获得最大值的下标，此处对应可能性最大的分类，axis=1意思是合并该行上所有的列\n",
    "        train_acc+=np.sum(np.argmax(train_pred.cpu().data.numpy(),axis=1)==data[1].numpy())\n",
    "        train_loss+=batch_loss.item()\n",
    "    model.eval()\n",
    "    # torch.no_grad() 是一个上下文管理器，被该语句 wrap 起来的部分将不会track梯度，将不再计算张量的梯度，跟踪张量的历史记录。这点在评估模型、测试模型阶段中常常用到，减少资源的使用\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}